{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet : Statistique en Grande Dimension - Exercice 1\n",
    "\n",
    "Solène Lesage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "#### Partie 2 : \n",
    "\n",
    "Dans cette partie, on souhaite tester différentes paramètres concernant comment régulariser en utilisant l'algorithme XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ilustration sur l'hyperparamètre : max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame({\"A\" : [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 9.0]})\n",
    "y_train = pd.DataFrame({\"Y\" : [2.0, 4.0, 6.0, 8.0, 20.0, 24.0, 28.0, 36.0]})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va faire varier max_depth pour voir l'impact de ce paramètre, les autres paramètres sont fixés à 0.\n",
    "C'est juste un exemple illustratif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.  8. 22. 22. 28. 36.]\n"
     ]
    }
   ],
   "source": [
    "# Choix de profondeur maximale à 3\n",
    "model = XGBRegressor(n_estimators=1,\n",
    "                     learning_rate=1.,\n",
    "                     base_score=0,\n",
    "                     max_depth=3,\n",
    "                     gamma=0,\n",
    "                     reg_alpha=0,\n",
    "                     reg_lambda=0)\n",
    "\n",
    "model.fit(x_train, y_train['Y'])\n",
    "pred = model.predict(x_train)\n",
    "print(pred) \n",
    "# -> [ 2.  4.  6.  8. 20. 24. 28. 36.]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce modèle ne permet pas un apprentissage complet et ne fournit pas une prédiction exacte comparé à l'échantillon y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.  8. 20. 24. 28. 36.]\n"
     ]
    }
   ],
   "source": [
    "# Choix de profondeur maximale à 4\n",
    "model1 = XGBRegressor(n_estimators=1,\n",
    "                     learning_rate=1.,\n",
    "                     base_score=0,\n",
    "                     max_depth=4,\n",
    "                     gamma=0,\n",
    "                     reg_alpha=0,\n",
    "                     reg_lambda=0)\n",
    "\n",
    "model1.fit(x_train, y_train['Y'])\n",
    "pred1 = model1.predict(x_train)\n",
    "print(pred1) \n",
    "# -> [ 2.  4.  6.  8. 20. 24. 28. 36.]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le second modèle avec un choix de profondeur maximale à 4, qui peut donc prédire jusqu'à 16 valeurs différentes (2^4) donne les prédictions souhaitées."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ilustration sur l'hyperparamètre : learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame({\"A\" : [3.0, 2.0, 1.0, 4.0, 5.0, 6.0, 7.0]})\n",
    "y_train = pd.DataFrame({\"Y\" : [3.0, 2.0, 1.0, 4.0, 5.0, 6.0, 7.0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 2. 1. 4. 5. 6. 7.]\n"
     ]
    }
   ],
   "source": [
    "# Learning rate égal à 1\n",
    "model = XGBRegressor(n_estimators=1,\n",
    "                     learning_rate=1.,\n",
    "                     base_score=0,\n",
    "                     max_depth=3,\n",
    "                     gamma=0,\n",
    "                     reg_alpha=0,\n",
    "                     reg_lambda=0)\n",
    "\n",
    "model.fit(x_train, y_train['Y'])\n",
    "pred = model.predict(x_train)\n",
    "print(pred) \n",
    "# -> [3. 2. 1. 4. 5. 6. 7.]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce modèle montre que les poids sont pas modifiés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.4 1.6 0.8 3.2 4.  4.8 5.6]\n"
     ]
    }
   ],
   "source": [
    "# Learning rate égal à 0.8\n",
    "model1 = XGBRegressor(n_estimators=1,\n",
    "                     learning_rate=0.8,\n",
    "                     base_score=0,\n",
    "                     max_depth=3,\n",
    "                     gamma=0,\n",
    "                     reg_alpha=0,\n",
    "                     reg_lambda=0)\n",
    "\n",
    "model1.fit(x_train, y_train['Y'])\n",
    "pred1 = model1.predict(x_train)\n",
    "print(pred1) \n",
    "# -> [2.4 1.6 0.8 3.2 4.  4.8 5.6]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit bien que les poids ont été multipliés par 0.8 (learning rate).\n",
    "Pour obtenir une meilleure prédiction, on pourrait augmenter le nombre d'arbres."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ilustration sur le paramètre de régularisation : gamma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code suivant utilise l'algorithme XGBoost pour classer un entier : soit il est positif soit négatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame({\"A\" : [-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0]})\n",
    "y_train = pd.DataFrame({\"Y\" : [-1, -1, -1, 1.0, 1.0, 1.0, 1.0]})\n",
    "y_train['Y'] = y_train['Y'] + np.random.normal(0, .1, y_train.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données d'entrainement associe -1 aux entiers négatifs et à 1 pour ceux positifs ou nuls.\n",
    "Le rajout de la loi normale est pour mettre en évidence l'exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9603168  -1.1339611  -0.98575723  1.1199436   0.944808    0.944808\n",
      "  1.1765499 ]\n"
     ]
    }
   ],
   "source": [
    "# overfitting\n",
    "model = XGBRegressor(n_estimators=1,\n",
    "                     learning_rate=1.,\n",
    "                     base_score=0,\n",
    "                     max_depth=3,\n",
    "                     gamma=0,\n",
    "                     reg_alpha=0,\n",
    "                     reg_lambda=0)\n",
    "\n",
    "model.fit(x_train, y_train['Y'])\n",
    "pred = model.predict(x_train)\n",
    "print(pred) \n",
    "# -> [-0.9603168  -1.1339611  -0.98575723  1.1199436   0.944808    0.944808 1.1765499 ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient 7 valeurs différentes et autant de feuilles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0266783 -1.0266783 -1.0266783  1.0465274  1.0465274  1.0465274\n",
      "  1.0465274]\n"
     ]
    }
   ],
   "source": [
    "# regularisation gamma = 1\n",
    "model1 = XGBRegressor(n_estimators=1,\n",
    "                     learning_rate=1.,\n",
    "                     base_score=0,\n",
    "                     max_depth=3,\n",
    "                     gamma=1,\n",
    "                     reg_alpha=0,\n",
    "                     reg_lambda=0)\n",
    "\n",
    "model1.fit(x_train, y_train['Y'])\n",
    "pred1 = model1.predict(x_train)\n",
    "print(pred1) \n",
    "# -> [-1.0266783 -1.0266783 -1.0266783  1.0465274  1.0465274  1.0465274 1.0465274]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ajout d'un niveau à l'arbre se fait que si le gain est supérieure à gamma ici 1.\n",
    "Les prédictions obtenues montrent seulement deux labels : -1.0266 et 1.0465.\n",
    "Le modèle a bien capturé le problème dans sa globalité qui a classer en seulement deux catégories les nombres : positifs et négatifs.\n",
    "On a bien réduit le sur-apprentissage en jouant sur la structure de l'arbre."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ilustration sur le paramètre de régularisation : lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9603168  -1.1339611  -0.98575723  1.1199436   0.944808    0.944808\n",
      "  1.1765499 ]\n"
     ]
    }
   ],
   "source": [
    "# sur-apprentissage évident avec lambda = 0\n",
    "model = XGBRegressor(n_estimators=1,\n",
    "                     learning_rate=1.,\n",
    "                     base_score=0,\n",
    "                     max_depth=3,\n",
    "                     gamma=0,\n",
    "                     reg_alpha=0,\n",
    "                     reg_lambda=0)\n",
    "\n",
    "model.fit(x_train, y_train['Y'])\n",
    "pred = model.predict(x_train)\n",
    "print(pred) \n",
    "# -> [-0.9603168  -1.1339611  -0.98575723  1.1199436   0.944808    0.944808 1.1765499 ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe un sur-apprentissage nette car chaque entrée a son propre label : deux labels suffiraient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7700088 -0.7700088 -0.7700088  0.8372219  0.8372219  0.8372219\n",
      "  0.8372219]\n"
     ]
    }
   ],
   "source": [
    "# regularisation lambda=1\n",
    "model1 = XGBRegressor(n_estimators=1,\n",
    "                     learning_rate=1.,\n",
    "                     base_score=0,\n",
    "                     max_depth=3,\n",
    "                     gamma=0,\n",
    "                     reg_alpha=0,\n",
    "                     reg_lambda=1)\n",
    "\n",
    "model1.fit(x_train, y_train['Y'])\n",
    "pred1 = model1.predict(x_train)\n",
    "print(pred1) \n",
    "# -> [-0.7700088 -0.7700088 -0.7700088  0.8372219  0.8372219  0.8372219 0.8372219]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le second modèle permet de limiter le sur-apprentissage et assure la création d'un modèle a 2 feuilles.\n",
    "Le paramètre lambda a une influence sur la structure du modèle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ilustration sur le paramètre de régularisation : alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "# Régularisation de type L2 : alpha = 1\n",
    "model = XGBClassifier(n_estimators=20,\n",
    "                      learning_rate=0.3,\n",
    "                      max_depth=3,\n",
    "                      gamma=0,\n",
    "                      reg_alpha=0,\n",
    "                      reg_lambda=2,\n",
    "                      eval_metric='mlogloss',\n",
    "                      use_label_encoder=False,\n",
    "                      num_class=3)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noeuds égaux à poids 0 : 0\n"
     ]
    }
   ],
   "source": [
    "# récupération des poids\n",
    "dtf = model.get_booster().trees_to_dataframe()\n",
    "dtf = dtf[dtf.Feature == 'Leaf']\n",
    "\n",
    "# récupération des poids proches de zéro\n",
    "dtf = dtf.sort_values(by=['Tree', 'Node', 'Gain'])\n",
    "print('Noeuds égaux à poids 0 :', dtf[abs(dtf.Gain) == 0.0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "# Régularisation de type L1 - modèle creux : alpha = 2\n",
    "model1 = XGBClassifier(n_estimators=20,\n",
    "                      learning_rate=0.3,\n",
    "                      max_depth=3,\n",
    "                      gamma=0,\n",
    "                      reg_alpha=2,\n",
    "                      reg_lambda=0,\n",
    "                      eval_metric='mlogloss',\n",
    "                      use_label_encoder=False,\n",
    "                      num_class=3)\n",
    "\n",
    "model1.fit(X_train, y_train)\n",
    "preds1 = model1.predict(X_test)\n",
    "print(confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noeuds égaux à poids 0 : 65\n"
     ]
    }
   ],
   "source": [
    "dtf1 = dtf\n",
    "\n",
    "# récupération des poids\n",
    "dtf = model1.get_booster().trees_to_dataframe()\n",
    "dtf = dtf[dtf.Feature == 'Leaf']\n",
    "# récupération des poids proches de zéro\n",
    "dtf = dtf.sort_values(by=['Tree', 'Node', 'Gain'])\n",
    "print('Noeuds égaux à poids 0 :', dtf[abs(dtf.Gain) == 0.0].shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a maintenant 65 poids qui ont une valeur nulle.\n",
    "\n",
    "Ainsi, le premier modèle utilise moins de poids nuls et utilise donc des valeurs très proches de 0.\n",
    "\n",
    "Tandis qu'en régularisant, avec un alpha positif, le modèle est beaucoup plus creux (contiennent bien des 0).\n",
    "\n",
    "On favorisera un modèle creux plutôt que dense car les 0 ne seront pas stockés, c'est donc un gain de stockage.\n",
    "\n",
    "Le fait de garder des poids différents de 0, les régularisations de type L1 pour alors sélectionner les caractéristiques les plus discriminantes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c229d494069f20c3fcb5c0373f6331941eaec1d792fc1562f17e0a0636a531a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
